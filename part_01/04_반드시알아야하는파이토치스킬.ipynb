{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. 반드시 알아야 하는 파이토치 스킬\n",
    "\n",
    "### 4.1 텐서\n",
    "텐서(Tensor) : 데이터를 표현하는 단위  \n",
    "\n",
    "#### 4.1.1 Scalar\n",
    "상숫값. 즉, 하나의 값을 표현할 때 1개의 수치로 표현한 것.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyongja/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/package/_mock_zipreader.py:17: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/torch/csrc/utils/tensor_numpy.cpp:67.)\n",
      "  _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "scalar1 = torch.tensor([1.])\n",
    "print(scalar1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.])\n"
     ]
    }
   ],
   "source": [
    "scalar2 = torch.tensor([3.])\n",
    "print(scalar2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스칼라 값 사칙연산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "add_scalar = scalar1 + scalar2\n",
    "print(add_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.])\n"
     ]
    }
   ],
   "source": [
    "sub_scalar = scalar1 - scalar2\n",
    "print(sub_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.])\n"
     ]
    }
   ],
   "source": [
    "mul_scalar = scalar1 * scalar2\n",
    "print(mul_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3333])\n"
     ]
    }
   ],
   "source": [
    "div_scalar = scalar1 / scalar2\n",
    "print(div_scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch 모듈에 내장된 메서드를 이용해 계산도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(scalar1, scalar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sub(scalar1, scalar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(scalar1, scalar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3333])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.div(scalar1, scalar2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Vector\n",
    "하나의 값을 표현할 때 2개 이상의 수치로 표현한 것. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "vector1 = torch.tensor([1., 2., 3.])\n",
    "print(vector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "vector2 = torch.tensor([4., 5., 6.])\n",
    "print(vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벡터 값 사칙연산 수행하기.  \n",
    "스칼라 사칙연산과 동일한 방식."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "add_vector = vector1 + vector2\n",
    "print(add_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3., -3., -3.])\n"
     ]
    }
   ],
   "source": [
    "sub_vector = vector1 - vector2\n",
    "print(sub_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4., 10., 18.])\n"
     ]
    }
   ],
   "source": [
    "mul_vector = vector1 * vector2\n",
    "print(mul_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2500, 0.4000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "div_vector = vector1 / vector2\n",
    "print(div_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 7., 9.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3., -3., -3.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sub(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4., 10., 18.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2500, 0.4000, 0.5000])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.div(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 행렬\n",
    "행렬은 2개 이상의 벡터 값을 통합해 구성된 값.  \n",
    "벡터 값 간 연산 속도를 빠르게 진행할 수 있는 선형 대수의 기본 단위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "matrix1 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 6.],\n",
      "        [7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "matrix2 = torch.tensor([[5., 6.], [7., 8.]])\n",
    "print(matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행렬 값 간의 사칙연산을 수행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n"
     ]
    }
   ],
   "source": [
    "sum_matrix = matrix1 + matrix2\n",
    "print(sum_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4., -4.],\n",
      "        [-4., -4.]])\n"
     ]
    }
   ],
   "source": [
    "sub_matrix = matrix1 - matrix2\n",
    "print(sub_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n"
     ]
    }
   ],
   "source": [
    "mul_matrix = matrix1 * matrix2\n",
    "print(mul_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2000, 0.3333],\n",
      "        [0.4286, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "div_matrix = matrix1 / matrix2\n",
    "print(div_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  8.],\n",
       "        [10., 12.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4., -4.],\n",
       "        [-4., -4.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sub(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5., 12.],\n",
       "        [21., 32.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2000, 0.3333],\n",
       "        [0.4286, 0.5000]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.div(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 22.],\n",
       "        [43., 50.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.4 텐서\n",
    "행렬을 2차원의 배열이라 표현할 수 있다면, 텐서는 2차원 이상의 배열이라 표현할 수 있음.  \n",
    "텐서 내 행렬 단위의 인덱스 간, 행렬 내 인덱스 간 원소끼리 게산되며 행렬 곱은 텐서 내 같은 행렬 단위의 인덱스 간에 계산 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.tensor([[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]]])\n",
    "print(tensor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 9., 10.],\n",
      "         [11., 12.]],\n",
      "\n",
      "        [[13., 14.],\n",
      "         [15., 16.]]])\n"
     ]
    }
   ],
   "source": [
    "tensor2 = torch.tensor([[[9., 10.], [11., 12.]], [[13., 14.], [15., 16.]]])\n",
    "print(tensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서의 사칙연산 수행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[10., 12.],\n",
      "         [14., 16.]],\n",
      "\n",
      "        [[18., 20.],\n",
      "         [22., 24.]]])\n"
     ]
    }
   ],
   "source": [
    "sum_tensor = tensor1 + tensor2\n",
    "print(sum_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-8., -8.],\n",
      "         [-8., -8.]],\n",
      "\n",
      "        [[-8., -8.],\n",
      "         [-8., -8.]]])\n"
     ]
    }
   ],
   "source": [
    "sub_tensor = tensor1 - tensor2\n",
    "print(sub_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  9.,  20.],\n",
      "         [ 33.,  48.]],\n",
      "\n",
      "        [[ 65.,  84.],\n",
      "         [105., 128.]]])\n"
     ]
    }
   ],
   "source": [
    "mul_tensor = tensor1 * tensor2\n",
    "print(mul_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1111, 0.2000],\n",
      "         [0.2727, 0.3333]],\n",
      "\n",
      "        [[0.3846, 0.4286],\n",
      "         [0.4667, 0.5000]]])\n"
     ]
    }
   ],
   "source": [
    "div_tensor = tensor1 / tensor2\n",
    "print(div_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[10., 12.],\n",
       "         [14., 16.]],\n",
       "\n",
       "        [[18., 20.],\n",
       "         [22., 24.]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-8., -8.],\n",
       "         [-8., -8.]],\n",
       "\n",
       "        [[-8., -8.],\n",
       "         [-8., -8.]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sub(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  9.,  20.],\n",
       "         [ 33.,  48.]],\n",
       "\n",
       "        [[ 65.,  84.],\n",
       "         [105., 128.]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1111, 0.2000],\n",
       "         [0.2727, 0.3333]],\n",
       "\n",
       "        [[0.3846, 0.4286],\n",
       "         [0.4667, 0.5000]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.div(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 31.,  34.],\n",
       "         [ 71.,  78.]],\n",
       "\n",
       "        [[155., 166.],\n",
       "         [211., 226.]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Autograd\n",
    "파이토치에서는 Back Propagation을 이용해 파라미터를 업데이트하는 방법을 Autograd 방식으로 쉽게 구현할 수 있음.  \n",
    "정말 간단한 딥러닝 모델을 설계하고 방정식 내에 존재하는 파라미터를 어떻게 업데이트할 수 있는지 알아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyongja/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() :\n",
    "    DEVICE = torch.device('cuda')\n",
    "else :\n",
    "    DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "INPUT_SIZE = 1000   \n",
    "HIDDEN_SIZE = 100\n",
    "OUTPUT_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BATCH_SIZE\n",
    "파라미터 업데이트할 때 계산되는 데이터 개수  \n",
    "BATCH_SIZE 수만큼 데이터를 이용해 Output 계산하고, BATCH_SIZE 수만큼 출력된 결괏값에 대한 오찻값 계산.  \n",
    "Batch size 수만큼 계산된 오찻값을 평균해 back propagation을 적용하고 이를 바탕으로 파라미터 업데이트.  \n",
    "이 예제에서 사용하는 batch size는 64이며, input으로 이용되는 데이터가 64개라는 것을 의미.  \n",
    "\n",
    "\n",
    "- INPUT_SIZE\n",
    "Input의 크기이자 입력층의 노드 수를 의미.  \n",
    "즉, batch size가 64이며 input이 1000이라는 것은 1000 크기의 벡터 값을 64개 이용한다는 의미. (64, 1000)\n",
    "\n",
    "\n",
    "- HIDDEN_SIZE\n",
    "딥러닝 모델에서 Input을 다수의 파라미터를 이용해 계산한 결과에 한 번 더 계산되는 파라미터 수를 의미.  \n",
    "즉, 입력층에서 은닉층으로 전달됐을 때 은닉층의 노드 수를 의미.  \n",
    "(64, 1000)의 input들이 (1000, 100) 크기의 행렬과 행렬 곱을 계산하기 위해 설정한 수.  \n",
    "\n",
    "\n",
    "- OUTPUT_SIZE\n",
    "최종으로 출력되는 값의 벡터의 크기를 의미함.  \n",
    "보통 output의 크기는 최종으로 비교하고자 하는 레이블의 크기와 동일하게 설정함.  \n",
    "예를 들어 10개로 분류하려면 크기가 10짜리의 one-hot encoding을 이용하기 때문에 output의 크기를 10으로 맞추기도 하고 5 크기의 벡터 값에 대해 mean squared error를 계산하기 위해 output의 크기를 '5'로 맞추기도 함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(BATCH_SIZE,\n",
    "               INPUT_SIZE,\n",
    "               device = DEVICE,\n",
    "               dtype = torch.float,\n",
    "               requires_grad = False)    # (1)\n",
    "y = torch.randn(BATCH_SIZE,\n",
    "               OUTPUT_SIZE,\n",
    "               device = DEVICE,\n",
    "               dtype = torch.float,\n",
    "               requires_grad =  False)    # (2)\n",
    "w1 = torch.randn(INPUT_SIZE,\n",
    "                HIDDEN_SIZE,\n",
    "                device = DEVICE,\n",
    "                dtype = torch.float,\n",
    "                requires_grad = True)    # (3)\n",
    "w2 = torch.randn(HIDDEN_SIZE,\n",
    "                 OUTPUT_SIZE,\n",
    "                 device = DEVICE,\n",
    "                 dtype = torch.float,\n",
    "                 requires_grad = True)    # (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) randn은 평균이 0, 표준편차가 1인 정규분포에서 샘플링한 값으로, 데이터를 만든다는 것을 의미하며 데이터를 만들어 낼 때 데이터의 모양을 설정할 수 있음. 즉, 크기가 1,000짜리의 벡터를 64개 만들기 위해 BATCH_SIZE는 '64', INPUT_SIZE는 '1,000'으로 설정했으며 x는 (64, 1000) 모양의 데이터가 생성됨. 또한 해당 데이터는 input으로 이용되기 때문에 gradient를 계산할 필요가 없음. (gradient는 파라미터 값을 업데이트하기 위해 계산하는 것이므로)  \n",
    "\n",
    "(2) Output도 input을 설정하는 것과 동일. batch size만큼 값이 필요하며 output과의 오차를 계산하기 위해 output의 크기를 10으로 설정  \n",
    "\n",
    "(3) w1은 input의 데이터 크기가 1000이며 이와 행렬 곱을 하기 위해 다음 행의 값이 1000이어야 함.  또한 행렬 곱을 이용해 100 크기의 데이터를 생헝하기 위해 (1000, 100) 크기의 데이터를 생성함. input, outpu과 달리 gradient를 계산할 수 있도록 설정.  \n",
    "\n",
    "(4) w2는 w1과 x를 행렬 곱한 결과에 계산할 수 있는 데이터여야 함.  w1과 x의 행렬 곱을 한 결과는 (64, 100)이며 (100, 10) 행렬을 통해 output을 계산할 수 있도록 모양 설정. gradient 계산할 수 있도록 설정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration :  100 \t Loss :  4.895027359452797e-06\n",
      "Iteration :  200 \t Loss :  4.1376561057404615e-06\n",
      "Iteration :  300 \t Loss :  3.596270744310459e-06\n",
      "Iteration :  400 \t Loss :  3.1195650080917403e-06\n",
      "Iteration :  500 \t Loss :  2.798250989144435e-06\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6                                              # (1)\n",
    "for t in range(1, 501) :                                          # (2)\n",
    "    y_pred = x.mm(w1).clamp(min = 0).mm(w2)                       # (3)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()                              # (4)\n",
    "    if t % 100 == 0 :\n",
    "        print(\"Iteration : \", t, \"\\t\", \"Loss : \", loss.item())    # (5)\n",
    "    loss.backward()                                               # (6)\n",
    "    \n",
    "    with torch.no_grad() :                                        # (7)\n",
    "        w1 -= learning_rate * w1.grad                             # (8)\n",
    "        w2 -= learning_rate * w2.grad                             # (9)\n",
    "        \n",
    "        w1.grad.zero_()                                           # (10)\n",
    "        w2.grad.zero_()                                           # (11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 파라미터 업데이트 시, gradient를 계산한 결과값에 1보다 작은 값을 곱해 업데이트.  \n",
    "\n",
    "(2) 500번 반복해 파라미터 값을 업데이트하기 위해 반복문 설정.  \n",
    "\n",
    "(3) 딥러닝 모델의 input인 x와 파라미터 w1 간의 행렬 곱을 이용해 나온 결괏값 계산. 그 이후 torch 모듈 내 clamp라는 메서드를 이용해 비선형 함수 적용. 딥러닝 모델에서는 층과 층 사이에 비선형 함수를 이용해 높은 표현력을 지니는 방정식을 얻을 수 있음. (ReLU()와 같은 역할!) clamp를 이용해 계산된 결과와 w2를 이용해 행렬 곱을 한 번 더 계산. 행렬 곱을 한 결과는 딥러닝 모델에서의 output을 의미하며 이는 예측값이므로 y_pred로 지정.  \n",
    "\n",
    "(4) 예측값과 실제 레이블 값을 비교해 오차를 계산한 값을 loss라고 함. y_pred와 실제 레이블을 의미하는 y간의 차잇값을 계산한 후 pow함수로 제곱 취한 후 sum()으로 제곱차의 합 구함.  \n",
    "\n",
    "(5) 반복 횟수를 의미하는 t가 100으로 나누어 떨어질때, 현재 진행 중인 반복문 횟수와 loss 값 출력해서 진행 과정 모니터링.  \n",
    "\n",
    "(6) 계산된 loss 값에 대해 backward()메서드를 이용하면 각 파라미터 값에 대해 gradient를 계산하고 이를 통해 back propagation을 진행 함.  \n",
    "\n",
    "(7) 각 파라미터 값에 대해 gradient를 계산한 결과를 이용해 파라미터 값을 업데이트 할 때는 해당 시점의 gradient 값을 고정한 후 업데이트를 진행함.  \n",
    "(8) gradient 값을 고정한 상태에서 w1의 gradient값을 의미하는 w1.grad에 (1)에서 설장한 learning_rate 값을 곱한 결괏값을 기존 w1에서 빼줌. 음수를 이요하는 이유는 loss 값이 최소로 계산될 수 있는 파라미터 값을 찾기 위해 gradient 값에 대한 반대 방향으로 계산한다는 것을 의미.  \n",
    "\n",
    "(9) (8)과 마찬가지로 w2에 대해서 진행\n",
    "\n",
    "(10), (11) (8), (9)를 통해 각 파라미터 값을 업데이트 했다면 각 파라미터 값의 gradient를 초기화해 다음 반복문을 진행할 수 있도록 gradient 값을 0으로 설정. w1, w2 각각에 대해 동일하게 grad.zero_() 메서드를 적용해 gradient 값을 0으로 설정. 다음 back_propagation을 진행할 때 gradient값을 loss.backward()을 통해 새로 계산하기 때문."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
