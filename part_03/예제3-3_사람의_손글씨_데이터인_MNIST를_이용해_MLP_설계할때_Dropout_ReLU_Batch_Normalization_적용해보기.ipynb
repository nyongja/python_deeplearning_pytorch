{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247ae8b4",
   "metadata": {},
   "source": [
    "## 예제 3-3) 사람의 손글씨 데이터인 MNIST를 이용해 MLP 설계할 때 Dropout & ReLU & Batch Normalization 적용해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f904cf4e",
   "metadata": {},
   "source": [
    "Batch Normalization은 각 Layer마다 Input의 분포가 달라짐에 따라 학습 속도가 현저히 느려지는 것을 방지하기 위해 이용되는 기법.  \n",
    "Batch Normalization은 1차원, 2차원, ... 등 다양한 차원에 따라 적용되는 함수명이 다르므로 유의해야 함.  \n",
    "MLP 내 각 layer에서 데이터는 1차원 크기의 벡터 값을 계산하므로 **nn.BatchNorm1d( )** 를 사용. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4af4c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn                           \n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d19b489f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version :  1.9.0  Device :  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyongja/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() :\n",
    "    DEVICE = torch.device('cuda')\n",
    "else :\n",
    "    DEVICE = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version : ', torch.__version__, ' Device : ', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "762a5b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a6f3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyongja/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                              train = True,\n",
    "                              download = True,\n",
    "                              transform = transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                              train = False,\n",
    "                              transform = transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a046fc2",
   "metadata": {},
   "source": [
    "nn.BatchNorm( ) 함수를 적용하는 부분은 논문 / 코드에 따라 Activation Function 전, 후가 달라질 수 있음.  \n",
    "이 예제에서는 이전에 적용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbe25794",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module) : \n",
    "    def __init__(self) :  \n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5\n",
    "        self.batch_norm1 = nn.BatchNorm1d(512)                                     # (1)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(256)                                     # (2)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm1(x)                                                    # (3)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batch_norm2(x)                                                    # (4)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42647381",
   "metadata": {},
   "source": [
    "(1) nn.BatchNorm() : Class 내에서 이용하기 위해 정의. 첫 번째 Fully Connected Layer의 Output이 512 크기의 벡터 값이므로 512 차원으로 설정  \n",
    "(2) nn.BatchNorm() : Class 내에서 이용하기 위해 정의. 두 번째 Fully Connected Layer의 Output이 256 크기의 벡터 값이므로 256 차원으로 설정  \n",
    "(3) 첫 번째 Fully Connected Layer의 Output을 'self.batch_norm1'의 input으로 이용  \n",
    "(4) 두 번째 Fully Connected Layer의 Output을 'self.batch_norm2'의 input으로 이용  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c414c945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(DEVICE)                                                      # (1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)    # (2)\n",
    "criterion = nn.CrossEntropyLoss()                                             # (3)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "174cda97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, log_interval) :\n",
    "    model.train()                                                                              # (1)\n",
    "    for batch_idx, (image, label) in enumerate(train_loader) :                                 # (2)\n",
    "        image = image.to(DEVICE)                                                               # (3)\n",
    "        label = label.to(DEVICE)                                                               # (4)\n",
    "        optimizer.zero_grad()                                                                  # (5)\n",
    "        output = model(image)                                                                  # (6)\n",
    "        loss = criterion(output, label)                                                        # (7)\n",
    "        loss.backward()                                                                        # (8)\n",
    "        optimizer.step()                                                                       # (9)\n",
    "        \n",
    "        if batch_idx % log_interval == 0 :\n",
    "            print(\"Train Eppoch : {} [{}/{}({:.0f}%)]\\tTrain Loss : {:.6f}\".format(\n",
    "                Epoch, batch_idx * len(image), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90930a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader) :\n",
    "    model.eval()                                                                     # (1)\n",
    "    test_loss = 0                                                                    # (2)\n",
    "    correct = 0                                                                      # (3)\n",
    "    \n",
    "    with torch.no_grad() :                                                           # (4)\n",
    "        for image, label in test_loader :                                            # (5)\n",
    "            image = image.to(DEVICE)                                                 # (6)\n",
    "            label = label.to(DEVICE)                                                 # (7)\n",
    "            output = model(image)                                                    # (8)\n",
    "            test_loss += criterion(output, label).item()                             # (9)\n",
    "            prediction = output.max(1, keepdim = True)[1]                            # (10)\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()         # (11)\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)                                            # (12)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)                        # (13)\n",
    "    return test_loss, test_accuracy                                                  # (14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d262451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Eppoch : 1 [0/60000(0%)]\tTrain Loss : 2.671281\n",
      "Train Eppoch : 1 [6400/60000(11%)]\tTrain Loss : 1.038421\n",
      "Train Eppoch : 1 [12800/60000(21%)]\tTrain Loss : 0.597079\n",
      "Train Eppoch : 1 [19200/60000(32%)]\tTrain Loss : 0.492205\n",
      "Train Eppoch : 1 [25600/60000(43%)]\tTrain Loss : 0.656568\n",
      "Train Eppoch : 1 [32000/60000(53%)]\tTrain Loss : 0.330573\n",
      "Train Eppoch : 1 [38400/60000(64%)]\tTrain Loss : 0.654169\n",
      "Train Eppoch : 1 [44800/60000(75%)]\tTrain Loss : 0.241623\n",
      "Train Eppoch : 1 [51200/60000(85%)]\tTrain Loss : 0.233484\n",
      "Train Eppoch : 1 [57600/60000(96%)]\tTrain Loss : 0.557291\n",
      "\n",
      "[EPOCH : 1], \tTest Loss : 0.0050, \tTest Accuracy : 94.95 %\n",
      "\n",
      "Train Eppoch : 2 [0/60000(0%)]\tTrain Loss : 0.197710\n",
      "Train Eppoch : 2 [6400/60000(11%)]\tTrain Loss : 0.350920\n",
      "Train Eppoch : 2 [12800/60000(21%)]\tTrain Loss : 0.301505\n",
      "Train Eppoch : 2 [19200/60000(32%)]\tTrain Loss : 0.138645\n",
      "Train Eppoch : 2 [25600/60000(43%)]\tTrain Loss : 0.274920\n",
      "Train Eppoch : 2 [32000/60000(53%)]\tTrain Loss : 0.369576\n",
      "Train Eppoch : 2 [38400/60000(64%)]\tTrain Loss : 0.206898\n",
      "Train Eppoch : 2 [44800/60000(75%)]\tTrain Loss : 0.360964\n",
      "Train Eppoch : 2 [51200/60000(85%)]\tTrain Loss : 0.287001\n",
      "Train Eppoch : 2 [57600/60000(96%)]\tTrain Loss : 0.188792\n",
      "\n",
      "[EPOCH : 2], \tTest Loss : 0.0036, \tTest Accuracy : 96.52 %\n",
      "\n",
      "Train Eppoch : 3 [0/60000(0%)]\tTrain Loss : 0.390510\n",
      "Train Eppoch : 3 [6400/60000(11%)]\tTrain Loss : 0.209169\n",
      "Train Eppoch : 3 [12800/60000(21%)]\tTrain Loss : 0.517900\n",
      "Train Eppoch : 3 [19200/60000(32%)]\tTrain Loss : 0.271225\n",
      "Train Eppoch : 3 [25600/60000(43%)]\tTrain Loss : 0.027943\n",
      "Train Eppoch : 3 [32000/60000(53%)]\tTrain Loss : 0.153234\n",
      "Train Eppoch : 3 [38400/60000(64%)]\tTrain Loss : 0.316268\n",
      "Train Eppoch : 3 [44800/60000(75%)]\tTrain Loss : 0.249157\n",
      "Train Eppoch : 3 [51200/60000(85%)]\tTrain Loss : 0.212242\n",
      "Train Eppoch : 3 [57600/60000(96%)]\tTrain Loss : 0.350430\n",
      "\n",
      "[EPOCH : 3], \tTest Loss : 0.0031, \tTest Accuracy : 96.88 %\n",
      "\n",
      "Train Eppoch : 4 [0/60000(0%)]\tTrain Loss : 0.170270\n",
      "Train Eppoch : 4 [6400/60000(11%)]\tTrain Loss : 0.019899\n",
      "Train Eppoch : 4 [12800/60000(21%)]\tTrain Loss : 0.272655\n",
      "Train Eppoch : 4 [19200/60000(32%)]\tTrain Loss : 0.163863\n",
      "Train Eppoch : 4 [25600/60000(43%)]\tTrain Loss : 0.138213\n",
      "Train Eppoch : 4 [32000/60000(53%)]\tTrain Loss : 0.098131\n",
      "Train Eppoch : 4 [38400/60000(64%)]\tTrain Loss : 0.127371\n",
      "Train Eppoch : 4 [44800/60000(75%)]\tTrain Loss : 0.138988\n",
      "Train Eppoch : 4 [51200/60000(85%)]\tTrain Loss : 0.371303\n",
      "Train Eppoch : 4 [57600/60000(96%)]\tTrain Loss : 0.248625\n",
      "\n",
      "[EPOCH : 4], \tTest Loss : 0.0028, \tTest Accuracy : 97.33 %\n",
      "\n",
      "Train Eppoch : 5 [0/60000(0%)]\tTrain Loss : 0.436685\n",
      "Train Eppoch : 5 [6400/60000(11%)]\tTrain Loss : 0.038674\n",
      "Train Eppoch : 5 [12800/60000(21%)]\tTrain Loss : 0.036270\n",
      "Train Eppoch : 5 [19200/60000(32%)]\tTrain Loss : 0.147822\n",
      "Train Eppoch : 5 [25600/60000(43%)]\tTrain Loss : 0.337014\n",
      "Train Eppoch : 5 [32000/60000(53%)]\tTrain Loss : 0.339335\n",
      "Train Eppoch : 5 [38400/60000(64%)]\tTrain Loss : 0.138691\n",
      "Train Eppoch : 5 [44800/60000(75%)]\tTrain Loss : 0.276760\n",
      "Train Eppoch : 5 [51200/60000(85%)]\tTrain Loss : 0.017465\n",
      "Train Eppoch : 5 [57600/60000(96%)]\tTrain Loss : 0.593943\n",
      "\n",
      "[EPOCH : 5], \tTest Loss : 0.0025, \tTest Accuracy : 97.41 %\n",
      "\n",
      "Train Eppoch : 6 [0/60000(0%)]\tTrain Loss : 0.133211\n",
      "Train Eppoch : 6 [6400/60000(11%)]\tTrain Loss : 0.138563\n",
      "Train Eppoch : 6 [12800/60000(21%)]\tTrain Loss : 0.283017\n",
      "Train Eppoch : 6 [19200/60000(32%)]\tTrain Loss : 0.058518\n",
      "Train Eppoch : 6 [25600/60000(43%)]\tTrain Loss : 0.217035\n",
      "Train Eppoch : 6 [32000/60000(53%)]\tTrain Loss : 0.188680\n",
      "Train Eppoch : 6 [38400/60000(64%)]\tTrain Loss : 0.168167\n",
      "Train Eppoch : 6 [44800/60000(75%)]\tTrain Loss : 0.350174\n",
      "Train Eppoch : 6 [51200/60000(85%)]\tTrain Loss : 0.030396\n",
      "Train Eppoch : 6 [57600/60000(96%)]\tTrain Loss : 0.268860\n",
      "\n",
      "[EPOCH : 6], \tTest Loss : 0.0023, \tTest Accuracy : 97.63 %\n",
      "\n",
      "Train Eppoch : 7 [0/60000(0%)]\tTrain Loss : 0.036302\n",
      "Train Eppoch : 7 [6400/60000(11%)]\tTrain Loss : 0.181363\n",
      "Train Eppoch : 7 [12800/60000(21%)]\tTrain Loss : 0.398697\n",
      "Train Eppoch : 7 [19200/60000(32%)]\tTrain Loss : 0.116342\n",
      "Train Eppoch : 7 [25600/60000(43%)]\tTrain Loss : 0.251242\n",
      "Train Eppoch : 7 [32000/60000(53%)]\tTrain Loss : 0.119012\n",
      "Train Eppoch : 7 [38400/60000(64%)]\tTrain Loss : 0.037168\n",
      "Train Eppoch : 7 [44800/60000(75%)]\tTrain Loss : 0.032406\n",
      "Train Eppoch : 7 [51200/60000(85%)]\tTrain Loss : 0.262158\n",
      "Train Eppoch : 7 [57600/60000(96%)]\tTrain Loss : 0.091978\n",
      "\n",
      "[EPOCH : 7], \tTest Loss : 0.0023, \tTest Accuracy : 97.72 %\n",
      "\n",
      "Train Eppoch : 8 [0/60000(0%)]\tTrain Loss : 0.138299\n",
      "Train Eppoch : 8 [6400/60000(11%)]\tTrain Loss : 0.085553\n",
      "Train Eppoch : 8 [12800/60000(21%)]\tTrain Loss : 0.258658\n",
      "Train Eppoch : 8 [19200/60000(32%)]\tTrain Loss : 0.085560\n",
      "Train Eppoch : 8 [25600/60000(43%)]\tTrain Loss : 0.059625\n",
      "Train Eppoch : 8 [32000/60000(53%)]\tTrain Loss : 0.182343\n",
      "Train Eppoch : 8 [38400/60000(64%)]\tTrain Loss : 0.073013\n",
      "Train Eppoch : 8 [44800/60000(75%)]\tTrain Loss : 0.090357\n",
      "Train Eppoch : 8 [51200/60000(85%)]\tTrain Loss : 0.045729\n",
      "Train Eppoch : 8 [57600/60000(96%)]\tTrain Loss : 0.066046\n",
      "\n",
      "[EPOCH : 8], \tTest Loss : 0.0021, \tTest Accuracy : 97.94 %\n",
      "\n",
      "Train Eppoch : 9 [0/60000(0%)]\tTrain Loss : 0.064536\n",
      "Train Eppoch : 9 [6400/60000(11%)]\tTrain Loss : 0.299859\n",
      "Train Eppoch : 9 [12800/60000(21%)]\tTrain Loss : 0.060852\n",
      "Train Eppoch : 9 [19200/60000(32%)]\tTrain Loss : 0.035335\n",
      "Train Eppoch : 9 [25600/60000(43%)]\tTrain Loss : 0.292346\n",
      "Train Eppoch : 9 [32000/60000(53%)]\tTrain Loss : 0.149199\n",
      "Train Eppoch : 9 [38400/60000(64%)]\tTrain Loss : 0.063269\n",
      "Train Eppoch : 9 [44800/60000(75%)]\tTrain Loss : 0.092220\n",
      "Train Eppoch : 9 [51200/60000(85%)]\tTrain Loss : 0.217425\n",
      "Train Eppoch : 9 [57600/60000(96%)]\tTrain Loss : 0.055517\n",
      "\n",
      "[EPOCH : 9], \tTest Loss : 0.0020, \tTest Accuracy : 97.97 %\n",
      "\n",
      "Train Eppoch : 10 [0/60000(0%)]\tTrain Loss : 0.068554\n",
      "Train Eppoch : 10 [6400/60000(11%)]\tTrain Loss : 0.117966\n",
      "Train Eppoch : 10 [12800/60000(21%)]\tTrain Loss : 0.191853\n",
      "Train Eppoch : 10 [19200/60000(32%)]\tTrain Loss : 0.172617\n",
      "Train Eppoch : 10 [25600/60000(43%)]\tTrain Loss : 0.227886\n",
      "Train Eppoch : 10 [32000/60000(53%)]\tTrain Loss : 0.041508\n",
      "Train Eppoch : 10 [38400/60000(64%)]\tTrain Loss : 0.093468\n",
      "Train Eppoch : 10 [44800/60000(75%)]\tTrain Loss : 0.158722\n",
      "Train Eppoch : 10 [51200/60000(85%)]\tTrain Loss : 0.022504\n",
      "Train Eppoch : 10 [57600/60000(96%)]\tTrain Loss : 0.203171\n",
      "\n",
      "[EPOCH : 10], \tTest Loss : 0.0020, \tTest Accuracy : 97.98 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for Epoch in range(1, EPOCHS + 1) :\n",
    "    train(model, train_loader, optimizer ,log_interval = 200)    # (1)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)      # (2)\n",
    "    print(\"\\n[EPOCH : {}], \\tTest Loss : {:.4f}, \\tTest Accuracy : {:.2f} %\\n\".format(Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b654610",
   "metadata": {},
   "source": [
    "Batch Normalization을 적용 했을 때 성능이 향상된 것을 확인 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baafee6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
