{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247ae8b4",
   "metadata": {},
   "source": [
    "## 예제 3-4) 사람의 손글씨 데이터인 MNIST를 이용해 MLP 설계할 때 Dropout & ReLU & Batch Normalization & He Uniform Initialization 적용해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f904cf4e",
   "metadata": {},
   "source": [
    "일반적인 딥러닝 모델은 다음 순서대로 설계해 학습하고 성능을 평가함.  \n",
    "1. 모델 구조를 설계하고 설계된 모델 구조의 파라미터 값을 랜덤으로 샘플링  \n",
    "2. Feature 값으로 이용되는 데이터를 설계한 모델의 Input으로 사용해 Output을 계산  \n",
    "3. 계산된 Output을 Input으로 이용한 Feature 값과 매칭되는 레이블 값을 기존에 정의한 objective function을 통해 Loss 값으로 계산  \n",
    "4. 계산된 Loss 값을 통해 Gradient를 계산해 모델 내 파라미터 값을 Back Propagation에 의해 업데이트  \n",
    "5. 이를 반복해 학습을 진행하며 완성된 모델의 성능 평가  \n",
    "\n",
    "이 중 1번에서 설계한 모델 구조의 파라미터 값을 랜덤으로 샘플링하는 과정에서 어떤 분포에서 샘플링을 진행하는지에 따라 모델의 학습이 좋은 방향으로 갈수도, 나쁜 방향으로 진행될 수도 있음.  \n",
    "-> 즉, 학습의 시작점을 좋게 설정하면 학습을 수월하게 할 수 있음!  \n",
    "\n",
    "현재 파이토치 내의 nn.linear는 Output으로 계산되는 벡터의 차원 수의 역수 값에 대한 +/- 범위 내 Uniform Distribution을 설정해 샘플링함.  \n",
    "이번 예제에서는, He Initialization으로 초기화해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4af4c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn                           \n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d19b489f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version :  1.9.0  Device :  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyongja/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() :\n",
    "    DEVICE = torch.device('cuda')\n",
    "else :\n",
    "    DEVICE = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version : ', torch.__version__, ' Device : ', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "762a5b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a6f3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyongja/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                              train = True,\n",
    "                              download = True,\n",
    "                              transform = transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                              train = False,\n",
    "                              transform = transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a046fc2",
   "metadata": {},
   "source": [
    "nn.BatchNorm( ) 함수를 적용하는 부분은 논문 / 코드에 따라 Activation Function 전, 후가 달라질 수 있음.  \n",
    "이 예제에서는 이전에 적용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbe25794",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module) : \n",
    "    def __init__(self) :  \n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5\n",
    "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9915057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ec1f9c",
   "metadata": {},
   "source": [
    "Weight, Bias 등 딥러닝 모델에서 초깃값으로 설정되는 요소에 대한 모듈인 init을 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b5f7f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m) :                              # (1)\n",
    "    if isinstance(m, nn.Linear) :                 # (2)\n",
    "        init.kaiming_uniform_(m.weight.data)      # (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7831beec",
   "metadata": {},
   "source": [
    "(1) MLP 모델 내의 Weight를 초기화할 부분을 설정하기 위한 함수 정의  \n",
    "(2) MLP 모델을 구성하고 있는 파라미터 중 nn.Linear에 해당하는 파라미터 값에 대해서만 지정  \n",
    "(3) nn.Linear에 해당하는 파라미터 값에 대해 he_initialization을 이용해 파라미터 값을 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c414c945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(DEVICE)\n",
    "model.apply(weight_init)                                                      # (1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411e1740",
   "metadata": {},
   "source": [
    "(1) 위에서 정의한 weight_init 함수를 Net( ) 클래스의 인스턴스인 model에 적용  \n",
    "\n",
    "지금까지 다룬 예제가 Class 내 모델을 설계하는 영역에서 설정했다면 초기값은 모델 정의하는 부분에서 설정 해야 함.  \n",
    "우선 model을 정의한 후 apply를 이용해 모델의 파라미터를 초기화.  \n",
    "초기화 진행 시, 정의된 weight_init 함수를 보면 모델 내 파라미터 값 중 nn.Linear 인스턴스에 대해서는 kaiming_uniform_을 이용해 초기화하는 것으로 설정돼있음. (kaming_uniform_ = He Initialization)  \n",
    "이외 파라미터 값은 기본값으로 설정된 분포에서 샘플링해 랜덤으로 설정됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "174cda97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, log_interval) :\n",
    "    model.train()            \n",
    "    for batch_idx, (image, label) in enumerate(train_loader) :\n",
    "        image = image.to(DEVICE)                                                               \n",
    "        label = label.to(DEVICE)                                                               \n",
    "        optimizer.zero_grad()                                                                  \n",
    "        output = model(image)                                                                  \n",
    "        loss = criterion(output, label)                                                        \n",
    "        loss.backward()                                                                        \n",
    "        optimizer.step()                                                                       \n",
    "        \n",
    "        if batch_idx % log_interval == 0 :\n",
    "            print(\"Train Eppoch : {} [{}/{}({:.0f}%)]\\tTrain Loss : {:.6f}\".format(\n",
    "                Epoch, batch_idx * len(image), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90930a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader) :\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad() :\n",
    "        for image, label in test_loader :\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)  \n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item() \n",
    "            \n",
    "    test_loss /= len(test_loader.dataset) \n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset) \n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d262451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Eppoch : 1 [0/60000(0%)]\tTrain Loss : 3.002760\n",
      "Train Eppoch : 1 [6400/60000(11%)]\tTrain Loss : 0.786273\n",
      "Train Eppoch : 1 [12800/60000(21%)]\tTrain Loss : 0.494366\n",
      "Train Eppoch : 1 [19200/60000(32%)]\tTrain Loss : 0.916311\n",
      "Train Eppoch : 1 [25600/60000(43%)]\tTrain Loss : 0.384322\n",
      "Train Eppoch : 1 [32000/60000(53%)]\tTrain Loss : 0.736573\n",
      "Train Eppoch : 1 [38400/60000(64%)]\tTrain Loss : 0.673953\n",
      "Train Eppoch : 1 [44800/60000(75%)]\tTrain Loss : 0.497907\n",
      "Train Eppoch : 1 [51200/60000(85%)]\tTrain Loss : 0.479082\n",
      "Train Eppoch : 1 [57600/60000(96%)]\tTrain Loss : 0.476468\n",
      "\n",
      "[EPOCH : 1], \tTest Loss : 0.0069, \tTest Accuracy : 93.47 %\n",
      "\n",
      "Train Eppoch : 2 [0/60000(0%)]\tTrain Loss : 0.423151\n",
      "Train Eppoch : 2 [6400/60000(11%)]\tTrain Loss : 0.363234\n",
      "Train Eppoch : 2 [12800/60000(21%)]\tTrain Loss : 0.235161\n",
      "Train Eppoch : 2 [19200/60000(32%)]\tTrain Loss : 0.676439\n",
      "Train Eppoch : 2 [25600/60000(43%)]\tTrain Loss : 0.159210\n",
      "Train Eppoch : 2 [32000/60000(53%)]\tTrain Loss : 0.452457\n",
      "Train Eppoch : 2 [38400/60000(64%)]\tTrain Loss : 0.711646\n",
      "Train Eppoch : 2 [44800/60000(75%)]\tTrain Loss : 0.343453\n",
      "Train Eppoch : 2 [51200/60000(85%)]\tTrain Loss : 0.236857\n",
      "Train Eppoch : 2 [57600/60000(96%)]\tTrain Loss : 0.104169\n",
      "\n",
      "[EPOCH : 2], \tTest Loss : 0.0053, \tTest Accuracy : 95.05 %\n",
      "\n",
      "Train Eppoch : 3 [0/60000(0%)]\tTrain Loss : 0.248201\n",
      "Train Eppoch : 3 [6400/60000(11%)]\tTrain Loss : 0.191507\n",
      "Train Eppoch : 3 [12800/60000(21%)]\tTrain Loss : 0.255329\n",
      "Train Eppoch : 3 [19200/60000(32%)]\tTrain Loss : 0.362368\n",
      "Train Eppoch : 3 [25600/60000(43%)]\tTrain Loss : 0.227873\n",
      "Train Eppoch : 3 [32000/60000(53%)]\tTrain Loss : 0.103111\n",
      "Train Eppoch : 3 [38400/60000(64%)]\tTrain Loss : 0.278244\n",
      "Train Eppoch : 3 [44800/60000(75%)]\tTrain Loss : 0.155676\n",
      "Train Eppoch : 3 [51200/60000(85%)]\tTrain Loss : 0.434242\n",
      "Train Eppoch : 3 [57600/60000(96%)]\tTrain Loss : 0.195967\n",
      "\n",
      "[EPOCH : 3], \tTest Loss : 0.0046, \tTest Accuracy : 95.49 %\n",
      "\n",
      "Train Eppoch : 4 [0/60000(0%)]\tTrain Loss : 0.247789\n",
      "Train Eppoch : 4 [6400/60000(11%)]\tTrain Loss : 0.251611\n",
      "Train Eppoch : 4 [12800/60000(21%)]\tTrain Loss : 0.352058\n",
      "Train Eppoch : 4 [19200/60000(32%)]\tTrain Loss : 0.173605\n",
      "Train Eppoch : 4 [25600/60000(43%)]\tTrain Loss : 0.146885\n",
      "Train Eppoch : 4 [32000/60000(53%)]\tTrain Loss : 0.275678\n",
      "Train Eppoch : 4 [38400/60000(64%)]\tTrain Loss : 0.165896\n",
      "Train Eppoch : 4 [44800/60000(75%)]\tTrain Loss : 0.413716\n",
      "Train Eppoch : 4 [51200/60000(85%)]\tTrain Loss : 0.485319\n",
      "Train Eppoch : 4 [57600/60000(96%)]\tTrain Loss : 0.097143\n",
      "\n",
      "[EPOCH : 4], \tTest Loss : 0.0040, \tTest Accuracy : 95.94 %\n",
      "\n",
      "Train Eppoch : 5 [0/60000(0%)]\tTrain Loss : 0.161830\n",
      "Train Eppoch : 5 [6400/60000(11%)]\tTrain Loss : 0.498854\n",
      "Train Eppoch : 5 [12800/60000(21%)]\tTrain Loss : 0.129031\n",
      "Train Eppoch : 5 [19200/60000(32%)]\tTrain Loss : 0.108065\n",
      "Train Eppoch : 5 [25600/60000(43%)]\tTrain Loss : 0.306631\n",
      "Train Eppoch : 5 [32000/60000(53%)]\tTrain Loss : 0.082749\n",
      "Train Eppoch : 5 [38400/60000(64%)]\tTrain Loss : 0.125920\n",
      "Train Eppoch : 5 [44800/60000(75%)]\tTrain Loss : 0.355760\n",
      "Train Eppoch : 5 [51200/60000(85%)]\tTrain Loss : 0.132293\n",
      "Train Eppoch : 5 [57600/60000(96%)]\tTrain Loss : 0.247080\n",
      "\n",
      "[EPOCH : 5], \tTest Loss : 0.0037, \tTest Accuracy : 96.27 %\n",
      "\n",
      "Train Eppoch : 6 [0/60000(0%)]\tTrain Loss : 0.295152\n",
      "Train Eppoch : 6 [6400/60000(11%)]\tTrain Loss : 0.316443\n",
      "Train Eppoch : 6 [12800/60000(21%)]\tTrain Loss : 0.383584\n",
      "Train Eppoch : 6 [19200/60000(32%)]\tTrain Loss : 0.217327\n",
      "Train Eppoch : 6 [25600/60000(43%)]\tTrain Loss : 0.177660\n",
      "Train Eppoch : 6 [32000/60000(53%)]\tTrain Loss : 0.260154\n",
      "Train Eppoch : 6 [38400/60000(64%)]\tTrain Loss : 0.414518\n",
      "Train Eppoch : 6 [44800/60000(75%)]\tTrain Loss : 0.064231\n",
      "Train Eppoch : 6 [51200/60000(85%)]\tTrain Loss : 0.081424\n",
      "Train Eppoch : 6 [57600/60000(96%)]\tTrain Loss : 0.290116\n",
      "\n",
      "[EPOCH : 6], \tTest Loss : 0.0035, \tTest Accuracy : 96.51 %\n",
      "\n",
      "Train Eppoch : 7 [0/60000(0%)]\tTrain Loss : 0.292058\n",
      "Train Eppoch : 7 [6400/60000(11%)]\tTrain Loss : 0.115419\n",
      "Train Eppoch : 7 [12800/60000(21%)]\tTrain Loss : 0.733285\n",
      "Train Eppoch : 7 [19200/60000(32%)]\tTrain Loss : 0.316665\n",
      "Train Eppoch : 7 [25600/60000(43%)]\tTrain Loss : 0.490771\n",
      "Train Eppoch : 7 [32000/60000(53%)]\tTrain Loss : 0.052618\n",
      "Train Eppoch : 7 [38400/60000(64%)]\tTrain Loss : 0.256018\n",
      "Train Eppoch : 7 [44800/60000(75%)]\tTrain Loss : 0.254255\n",
      "Train Eppoch : 7 [51200/60000(85%)]\tTrain Loss : 0.151427\n",
      "Train Eppoch : 7 [57600/60000(96%)]\tTrain Loss : 0.178724\n",
      "\n",
      "[EPOCH : 7], \tTest Loss : 0.0032, \tTest Accuracy : 96.75 %\n",
      "\n",
      "Train Eppoch : 8 [0/60000(0%)]\tTrain Loss : 0.286889\n",
      "Train Eppoch : 8 [6400/60000(11%)]\tTrain Loss : 0.153653\n",
      "Train Eppoch : 8 [12800/60000(21%)]\tTrain Loss : 0.659774\n",
      "Train Eppoch : 8 [19200/60000(32%)]\tTrain Loss : 0.216758\n",
      "Train Eppoch : 8 [25600/60000(43%)]\tTrain Loss : 0.294467\n",
      "Train Eppoch : 8 [32000/60000(53%)]\tTrain Loss : 0.374726\n",
      "Train Eppoch : 8 [38400/60000(64%)]\tTrain Loss : 0.123939\n",
      "Train Eppoch : 8 [44800/60000(75%)]\tTrain Loss : 0.381267\n",
      "Train Eppoch : 8 [51200/60000(85%)]\tTrain Loss : 0.369442\n",
      "Train Eppoch : 8 [57600/60000(96%)]\tTrain Loss : 0.140102\n",
      "\n",
      "[EPOCH : 8], \tTest Loss : 0.0031, \tTest Accuracy : 96.86 %\n",
      "\n",
      "Train Eppoch : 9 [0/60000(0%)]\tTrain Loss : 0.070265\n",
      "Train Eppoch : 9 [6400/60000(11%)]\tTrain Loss : 0.279463\n",
      "Train Eppoch : 9 [12800/60000(21%)]\tTrain Loss : 0.157486\n",
      "Train Eppoch : 9 [19200/60000(32%)]\tTrain Loss : 0.339199\n",
      "Train Eppoch : 9 [25600/60000(43%)]\tTrain Loss : 0.326940\n",
      "Train Eppoch : 9 [32000/60000(53%)]\tTrain Loss : 0.150618\n",
      "Train Eppoch : 9 [38400/60000(64%)]\tTrain Loss : 0.068285\n",
      "Train Eppoch : 9 [44800/60000(75%)]\tTrain Loss : 0.204893\n",
      "Train Eppoch : 9 [51200/60000(85%)]\tTrain Loss : 0.018095\n",
      "Train Eppoch : 9 [57600/60000(96%)]\tTrain Loss : 0.183457\n",
      "\n",
      "[EPOCH : 9], \tTest Loss : 0.0028, \tTest Accuracy : 97.28 %\n",
      "\n",
      "Train Eppoch : 10 [0/60000(0%)]\tTrain Loss : 0.515515\n",
      "Train Eppoch : 10 [6400/60000(11%)]\tTrain Loss : 0.288337\n",
      "Train Eppoch : 10 [12800/60000(21%)]\tTrain Loss : 0.309626\n",
      "Train Eppoch : 10 [19200/60000(32%)]\tTrain Loss : 0.086620\n",
      "Train Eppoch : 10 [25600/60000(43%)]\tTrain Loss : 0.078030\n",
      "Train Eppoch : 10 [32000/60000(53%)]\tTrain Loss : 0.121242\n",
      "Train Eppoch : 10 [38400/60000(64%)]\tTrain Loss : 0.413283\n",
      "Train Eppoch : 10 [44800/60000(75%)]\tTrain Loss : 0.074855\n",
      "Train Eppoch : 10 [51200/60000(85%)]\tTrain Loss : 0.044421\n",
      "Train Eppoch : 10 [57600/60000(96%)]\tTrain Loss : 0.265589\n",
      "\n",
      "[EPOCH : 10], \tTest Loss : 0.0028, \tTest Accuracy : 97.17 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for Epoch in range(1, EPOCHS + 1) :\n",
    "    train(model, train_loader, optimizer ,log_interval = 200)    # (1)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)      # (2)\n",
    "    print(\"\\n[EPOCH : {}], \\tTest Loss : {:.4f}, \\tTest Accuracy : {:.2f} %\\n\".format(Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baafee6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
