{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247ae8b4",
   "metadata": {},
   "source": [
    "## 예제 3-2) 사람의 손글씨 데이터인 MNIST를 이용해 MLP 설계할 때 Dropout & ReLU 적용해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f904cf4e",
   "metadata": {},
   "source": [
    "sigmoid()를 relu()로 변경해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4af4c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn                           \n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d19b489f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version :  1.9.0  Device :  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyongja/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() :\n",
    "    DEVICE = torch.device('cuda')\n",
    "else :\n",
    "    DEVICE = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version : ', torch.__version__, ' Device : ', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "762a5b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2a6f3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyongja/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                              train = True,\n",
    "                              download = True,\n",
    "                              transform = transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                              train = False,\n",
    "                              transform = transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbe25794",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module) : \n",
    "    def __init__(self) :  \n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)                                                              # (1)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)                                                              # (2)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42647381",
   "metadata": {},
   "source": [
    "ReLU() 함수는 0 미만인 값은 0으로, 양수 값은 그대로 반영하는 비선형 함수이며 Gradient를 빠르게 계산하고 Back Propagation을 효과적으로 이용할 수 있으므로 많은 딥러닝 모형을 설계할 때 많이 이용됨.  \n",
    "이와 반대로 sigmoid() 비선형 함수는 0에서 멀어질수록 Gradient 값이 0에 가까워 Back Propagation이 효과적으로 이용되기 어려움. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c414c945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(DEVICE)                                                      # (1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)    # (2)\n",
    "criterion = nn.CrossEntropyLoss()                                             # (3)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "174cda97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, log_interval) :\n",
    "    model.train()                                                                              # (1)\n",
    "    for batch_idx, (image, label) in enumerate(train_loader) :                                 # (2)\n",
    "        image = image.to(DEVICE)                                                               # (3)\n",
    "        label = label.to(DEVICE)                                                               # (4)\n",
    "        optimizer.zero_grad()                                                                  # (5)\n",
    "        output = model(image)                                                                  # (6)\n",
    "        loss = criterion(output, label)                                                        # (7)\n",
    "        loss.backward()                                                                        # (8)\n",
    "        optimizer.step()                                                                       # (9)\n",
    "        \n",
    "        if batch_idx % log_interval == 0 :\n",
    "            print(\"Train Eppoch : {} [{}/{}({:.0f}%)]\\tTrain Loss : {:.6f}\".format(\n",
    "                Epoch, batch_idx * len(image), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90930a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader) :\n",
    "    model.eval()                                                                     # (1)\n",
    "    test_loss = 0                                                                    # (2)\n",
    "    correct = 0                                                                      # (3)\n",
    "    \n",
    "    with torch.no_grad() :                                                           # (4)\n",
    "        for image, label in test_loader :                                            # (5)\n",
    "            image = image.to(DEVICE)                                                 # (6)\n",
    "            label = label.to(DEVICE)                                                 # (7)\n",
    "            output = model(image)                                                    # (8)\n",
    "            test_loss += criterion(output, label).item()                             # (9)\n",
    "            prediction = output.max(1, keepdim = True)[1]                            # (10)\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()         # (11)\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)                                            # (12)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)                        # (13)\n",
    "    return test_loss, test_accuracy                                                  # (14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d262451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Eppoch : 1 [0/60000(0%)]\tTrain Loss : 2.298302\n",
      "Train Eppoch : 1 [6400/60000(11%)]\tTrain Loss : 2.068712\n",
      "Train Eppoch : 1 [12800/60000(21%)]\tTrain Loss : 1.184849\n",
      "Train Eppoch : 1 [19200/60000(32%)]\tTrain Loss : 0.927167\n",
      "Train Eppoch : 1 [25600/60000(43%)]\tTrain Loss : 0.742161\n",
      "Train Eppoch : 1 [32000/60000(53%)]\tTrain Loss : 0.420508\n",
      "Train Eppoch : 1 [38400/60000(64%)]\tTrain Loss : 0.475219\n",
      "Train Eppoch : 1 [44800/60000(75%)]\tTrain Loss : 0.601190\n",
      "Train Eppoch : 1 [51200/60000(85%)]\tTrain Loss : 0.607545\n",
      "Train Eppoch : 1 [57600/60000(96%)]\tTrain Loss : 0.584671\n",
      "\n",
      "[EPOCH : 1], \tTest Loss : 0.0099, \tTest Accuracy : 91.08 %\n",
      "\n",
      "Train Eppoch : 2 [0/60000(0%)]\tTrain Loss : 0.375350\n",
      "Train Eppoch : 2 [6400/60000(11%)]\tTrain Loss : 0.378901\n",
      "Train Eppoch : 2 [12800/60000(21%)]\tTrain Loss : 0.375031\n",
      "Train Eppoch : 2 [19200/60000(32%)]\tTrain Loss : 0.349537\n",
      "Train Eppoch : 2 [25600/60000(43%)]\tTrain Loss : 0.467491\n",
      "Train Eppoch : 2 [32000/60000(53%)]\tTrain Loss : 0.423832\n",
      "Train Eppoch : 2 [38400/60000(64%)]\tTrain Loss : 0.179629\n",
      "Train Eppoch : 2 [44800/60000(75%)]\tTrain Loss : 0.208272\n",
      "Train Eppoch : 2 [51200/60000(85%)]\tTrain Loss : 0.256701\n",
      "Train Eppoch : 2 [57600/60000(96%)]\tTrain Loss : 0.187764\n",
      "\n",
      "[EPOCH : 2], \tTest Loss : 0.0070, \tTest Accuracy : 93.40 %\n",
      "\n",
      "Train Eppoch : 3 [0/60000(0%)]\tTrain Loss : 0.133608\n",
      "Train Eppoch : 3 [6400/60000(11%)]\tTrain Loss : 0.574927\n",
      "Train Eppoch : 3 [12800/60000(21%)]\tTrain Loss : 0.504366\n",
      "Train Eppoch : 3 [19200/60000(32%)]\tTrain Loss : 0.090232\n",
      "Train Eppoch : 3 [25600/60000(43%)]\tTrain Loss : 0.380256\n",
      "Train Eppoch : 3 [32000/60000(53%)]\tTrain Loss : 0.187318\n",
      "Train Eppoch : 3 [38400/60000(64%)]\tTrain Loss : 0.079808\n",
      "Train Eppoch : 3 [44800/60000(75%)]\tTrain Loss : 0.233799\n",
      "Train Eppoch : 3 [51200/60000(85%)]\tTrain Loss : 0.112022\n",
      "Train Eppoch : 3 [57600/60000(96%)]\tTrain Loss : 0.543787\n",
      "\n",
      "[EPOCH : 3], \tTest Loss : 0.0055, \tTest Accuracy : 94.65 %\n",
      "\n",
      "Train Eppoch : 4 [0/60000(0%)]\tTrain Loss : 0.306020\n",
      "Train Eppoch : 4 [6400/60000(11%)]\tTrain Loss : 0.392486\n",
      "Train Eppoch : 4 [12800/60000(21%)]\tTrain Loss : 0.333895\n",
      "Train Eppoch : 4 [19200/60000(32%)]\tTrain Loss : 0.180499\n",
      "Train Eppoch : 4 [25600/60000(43%)]\tTrain Loss : 0.349732\n",
      "Train Eppoch : 4 [32000/60000(53%)]\tTrain Loss : 0.208767\n",
      "Train Eppoch : 4 [38400/60000(64%)]\tTrain Loss : 0.292910\n",
      "Train Eppoch : 4 [44800/60000(75%)]\tTrain Loss : 0.130026\n",
      "Train Eppoch : 4 [51200/60000(85%)]\tTrain Loss : 0.146152\n",
      "Train Eppoch : 4 [57600/60000(96%)]\tTrain Loss : 0.255811\n",
      "\n",
      "[EPOCH : 4], \tTest Loss : 0.0045, \tTest Accuracy : 95.53 %\n",
      "\n",
      "Train Eppoch : 5 [0/60000(0%)]\tTrain Loss : 0.202216\n",
      "Train Eppoch : 5 [6400/60000(11%)]\tTrain Loss : 0.228990\n",
      "Train Eppoch : 5 [12800/60000(21%)]\tTrain Loss : 0.326827\n",
      "Train Eppoch : 5 [19200/60000(32%)]\tTrain Loss : 0.184572\n",
      "Train Eppoch : 5 [25600/60000(43%)]\tTrain Loss : 0.372651\n",
      "Train Eppoch : 5 [32000/60000(53%)]\tTrain Loss : 0.301742\n",
      "Train Eppoch : 5 [38400/60000(64%)]\tTrain Loss : 0.360575\n",
      "Train Eppoch : 5 [44800/60000(75%)]\tTrain Loss : 0.066262\n",
      "Train Eppoch : 5 [51200/60000(85%)]\tTrain Loss : 0.219378\n",
      "Train Eppoch : 5 [57600/60000(96%)]\tTrain Loss : 0.268800\n",
      "\n",
      "[EPOCH : 5], \tTest Loss : 0.0041, \tTest Accuracy : 96.00 %\n",
      "\n",
      "Train Eppoch : 6 [0/60000(0%)]\tTrain Loss : 0.363629\n",
      "Train Eppoch : 6 [6400/60000(11%)]\tTrain Loss : 0.219471\n",
      "Train Eppoch : 6 [12800/60000(21%)]\tTrain Loss : 0.021570\n",
      "Train Eppoch : 6 [19200/60000(32%)]\tTrain Loss : 0.306883\n",
      "Train Eppoch : 6 [25600/60000(43%)]\tTrain Loss : 0.064843\n",
      "Train Eppoch : 6 [32000/60000(53%)]\tTrain Loss : 0.069160\n",
      "Train Eppoch : 6 [38400/60000(64%)]\tTrain Loss : 0.115531\n",
      "Train Eppoch : 6 [44800/60000(75%)]\tTrain Loss : 0.027397\n",
      "Train Eppoch : 6 [51200/60000(85%)]\tTrain Loss : 0.348828\n",
      "Train Eppoch : 6 [57600/60000(96%)]\tTrain Loss : 0.046096\n",
      "\n",
      "[EPOCH : 6], \tTest Loss : 0.0036, \tTest Accuracy : 96.38 %\n",
      "\n",
      "Train Eppoch : 7 [0/60000(0%)]\tTrain Loss : 0.072759\n",
      "Train Eppoch : 7 [6400/60000(11%)]\tTrain Loss : 0.099200\n",
      "Train Eppoch : 7 [12800/60000(21%)]\tTrain Loss : 0.121769\n",
      "Train Eppoch : 7 [19200/60000(32%)]\tTrain Loss : 0.092930\n",
      "Train Eppoch : 7 [25600/60000(43%)]\tTrain Loss : 0.069963\n",
      "Train Eppoch : 7 [32000/60000(53%)]\tTrain Loss : 0.146370\n",
      "Train Eppoch : 7 [38400/60000(64%)]\tTrain Loss : 0.320657\n",
      "Train Eppoch : 7 [44800/60000(75%)]\tTrain Loss : 0.174861\n",
      "Train Eppoch : 7 [51200/60000(85%)]\tTrain Loss : 0.071854\n",
      "Train Eppoch : 7 [57600/60000(96%)]\tTrain Loss : 0.072249\n",
      "\n",
      "[EPOCH : 7], \tTest Loss : 0.0033, \tTest Accuracy : 96.57 %\n",
      "\n",
      "Train Eppoch : 8 [0/60000(0%)]\tTrain Loss : 0.079492\n",
      "Train Eppoch : 8 [6400/60000(11%)]\tTrain Loss : 0.053309\n",
      "Train Eppoch : 8 [12800/60000(21%)]\tTrain Loss : 0.022485\n",
      "Train Eppoch : 8 [19200/60000(32%)]\tTrain Loss : 0.103284\n",
      "Train Eppoch : 8 [25600/60000(43%)]\tTrain Loss : 0.155775\n",
      "Train Eppoch : 8 [32000/60000(53%)]\tTrain Loss : 0.119720\n",
      "Train Eppoch : 8 [38400/60000(64%)]\tTrain Loss : 0.059686\n",
      "Train Eppoch : 8 [44800/60000(75%)]\tTrain Loss : 0.048466\n",
      "Train Eppoch : 8 [51200/60000(85%)]\tTrain Loss : 0.050471\n",
      "Train Eppoch : 8 [57600/60000(96%)]\tTrain Loss : 0.070977\n",
      "\n",
      "[EPOCH : 8], \tTest Loss : 0.0030, \tTest Accuracy : 96.98 %\n",
      "\n",
      "Train Eppoch : 9 [0/60000(0%)]\tTrain Loss : 0.143086\n",
      "Train Eppoch : 9 [6400/60000(11%)]\tTrain Loss : 0.090012\n",
      "Train Eppoch : 9 [12800/60000(21%)]\tTrain Loss : 0.035672\n",
      "Train Eppoch : 9 [19200/60000(32%)]\tTrain Loss : 0.103368\n",
      "Train Eppoch : 9 [25600/60000(43%)]\tTrain Loss : 0.103867\n",
      "Train Eppoch : 9 [32000/60000(53%)]\tTrain Loss : 0.058052\n",
      "Train Eppoch : 9 [38400/60000(64%)]\tTrain Loss : 0.044755\n",
      "Train Eppoch : 9 [44800/60000(75%)]\tTrain Loss : 0.043524\n",
      "Train Eppoch : 9 [51200/60000(85%)]\tTrain Loss : 0.062592\n",
      "Train Eppoch : 9 [57600/60000(96%)]\tTrain Loss : 0.088890\n",
      "\n",
      "[EPOCH : 9], \tTest Loss : 0.0028, \tTest Accuracy : 97.30 %\n",
      "\n",
      "Train Eppoch : 10 [0/60000(0%)]\tTrain Loss : 0.053581\n",
      "Train Eppoch : 10 [6400/60000(11%)]\tTrain Loss : 0.100039\n",
      "Train Eppoch : 10 [12800/60000(21%)]\tTrain Loss : 0.069821\n",
      "Train Eppoch : 10 [19200/60000(32%)]\tTrain Loss : 0.029978\n",
      "Train Eppoch : 10 [25600/60000(43%)]\tTrain Loss : 0.066594\n",
      "Train Eppoch : 10 [32000/60000(53%)]\tTrain Loss : 0.157850\n",
      "Train Eppoch : 10 [38400/60000(64%)]\tTrain Loss : 0.057727\n",
      "Train Eppoch : 10 [44800/60000(75%)]\tTrain Loss : 0.145335\n",
      "Train Eppoch : 10 [51200/60000(85%)]\tTrain Loss : 0.277683\n",
      "Train Eppoch : 10 [57600/60000(96%)]\tTrain Loss : 0.061886\n",
      "\n",
      "[EPOCH : 10], \tTest Loss : 0.0027, \tTest Accuracy : 97.38 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for Epoch in range(1, EPOCHS + 1) :\n",
    "    train(model, train_loader, optimizer ,log_interval = 200)    # (1)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)      # (2)\n",
    "    print(\"\\n[EPOCH : {}], \\tTest Loss : {:.4f}, \\tTest Accuracy : {:.2f} %\\n\".format(Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b654610",
   "metadata": {},
   "source": [
    "sigmoid() 함수를 적용했을 때보다 ReLU() 적용해 비선형 함수를 이용했을 때 학습시작부터 높은 성능을 유지하며 학습이 진행될수록 성능이 더욱 좋아짐."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baafee6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
