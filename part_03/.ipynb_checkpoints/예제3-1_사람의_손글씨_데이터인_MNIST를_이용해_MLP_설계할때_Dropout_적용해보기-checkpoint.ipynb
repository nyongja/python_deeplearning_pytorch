{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247ae8b4",
   "metadata": {},
   "source": [
    "## 예제 3-1) 사람의 손글씨 데이터인 MNIST를 이용해 MLP 설계할 때 Dropout 적용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4af4c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn                           # (1)\n",
    "import torch.nn.functional as F                 # (2)\n",
    "from torchvision import transforms, datasets    # (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d19b489f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version :  1.9.0  Device :  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyongja/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() :\n",
    "    DEVICE = torch.device('cuda')\n",
    "else :\n",
    "    DEVICE = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version : ', torch.__version__, ' Device : ', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "762a5b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32    # (1)\n",
    "EPOCHS = 10        # (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2a6f3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyongja/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST(root = \"../data/MNIST\",                 #(1)\n",
    "                              train = True,\n",
    "                              download = True,\n",
    "                              transform = transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root = \"../data/MNIST\",                  #(2)\n",
    "                              train = False,\n",
    "                              transform = transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,    #(3)\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,     #(4)\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45e3677",
   "metadata": {},
   "source": [
    "파이토치를 이용하면 Dropout 적용하는 것은 매우 간단함.  \n",
    "Part 02 예제에서 MLP 모델 설계 부분에 Dropout을 추가해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbe25794",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module) : \n",
    "    def __init__(self) :  \n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5                                              # (1)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)    # (2)\n",
    "        x = self.fc2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)    # (3)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42647381",
   "metadata": {},
   "source": [
    "(1) 몇 퍼센트의 노드에 대해 가중값을 계산하지 않을 것인지를 명시.  \n",
    "(2), (3) 각 sigmoid() 함수의 결괏값에 대해 Dropout을 적용하는 부분. \n",
    "- p : 몇 퍼센트의 노드에 대해 계산하지 않을 것인지를 조정하는 요소. \n",
    "- training = self.training : 학습 상태일 때와 검증 상태에 따라 다르게 적용되기 위해 존재하는 파라미터.  Dropout은 학습 과정 속에서 랜덤으로 노드를 선택해 가중값이 업데이트되지 않도록 조정하지만, 평가 과정 속에서는 모든 노드를 이용해 Output을 계산하기 때문에 학습 상태와 검증 상태에서 다르게 적용돼야 함. 이를 반영하기 위한 파라미터 값을 model.train()으로 명시할 때 self.training = True, model.eval()로 명시할 때, self.training = False로 적용됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c414c945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(DEVICE)                                                      # (1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)    # (2)\n",
    "criterion = nn.CrossEntropyLoss()                                             # (3)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "174cda97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, log_interval) :\n",
    "    model.train()                                                                              # (1)\n",
    "    for batch_idx, (image, label) in enumerate(train_loader) :                                 # (2)\n",
    "        image = image.to(DEVICE)                                                               # (3)\n",
    "        label = label.to(DEVICE)                                                               # (4)\n",
    "        optimizer.zero_grad()                                                                  # (5)\n",
    "        output = model(image)                                                                  # (6)\n",
    "        loss = criterion(output, label)                                                        # (7)\n",
    "        loss.backward()                                                                        # (8)\n",
    "        optimizer.step()                                                                       # (9)\n",
    "        \n",
    "        if batch_idx % log_interval == 0 :\n",
    "            print(\"Train Eppoch : {} [{}/{}({:.0f}%)]\\tTrain Loss : {:.6f}\".format(\n",
    "                Epoch, batch_idx * len(image), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90930a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader) :\n",
    "    model.eval()                                                                     # (1)\n",
    "    test_loss = 0                                                                    # (2)\n",
    "    correct = 0                                                                      # (3)\n",
    "    \n",
    "    with torch.no_grad() :                                                           # (4)\n",
    "        for image, label in test_loader :                                            # (5)\n",
    "            image = image.to(DEVICE)                                                 # (6)\n",
    "            label = label.to(DEVICE)                                                 # (7)\n",
    "            output = model(image)                                                    # (8)\n",
    "            test_loss += criterion(output, label).item()                             # (9)\n",
    "            prediction = output.max(1, keepdim = True)[1]                            # (10)\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()         # (11)\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)                                            # (12)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)                        # (13)\n",
    "    return test_loss, test_accuracy                                                  # (14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d262451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyongja/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Eppoch : 1 [0/60000(0%)]\tTrain Loss : 2.337806\n",
      "Train Eppoch : 1 [6400/60000(11%)]\tTrain Loss : 2.378829\n",
      "Train Eppoch : 1 [12800/60000(21%)]\tTrain Loss : 2.330299\n",
      "Train Eppoch : 1 [19200/60000(32%)]\tTrain Loss : 2.320760\n",
      "Train Eppoch : 1 [25600/60000(43%)]\tTrain Loss : 2.306573\n",
      "Train Eppoch : 1 [32000/60000(53%)]\tTrain Loss : 2.331651\n",
      "Train Eppoch : 1 [38400/60000(64%)]\tTrain Loss : 2.300152\n",
      "Train Eppoch : 1 [44800/60000(75%)]\tTrain Loss : 2.297417\n",
      "Train Eppoch : 1 [51200/60000(85%)]\tTrain Loss : 2.323766\n",
      "Train Eppoch : 1 [57600/60000(96%)]\tTrain Loss : 2.317085\n",
      "\n",
      "[EPOCH : 1], \tTest Loss : 0.0714, \tTest Accuracy : 9.64 %\n",
      "\n",
      "Train Eppoch : 2 [0/60000(0%)]\tTrain Loss : 2.325015\n",
      "Train Eppoch : 2 [6400/60000(11%)]\tTrain Loss : 2.244712\n",
      "Train Eppoch : 2 [12800/60000(21%)]\tTrain Loss : 2.284172\n",
      "Train Eppoch : 2 [19200/60000(32%)]\tTrain Loss : 2.303686\n",
      "Train Eppoch : 2 [25600/60000(43%)]\tTrain Loss : 2.262102\n",
      "Train Eppoch : 2 [32000/60000(53%)]\tTrain Loss : 2.203714\n",
      "Train Eppoch : 2 [38400/60000(64%)]\tTrain Loss : 2.203285\n",
      "Train Eppoch : 2 [44800/60000(75%)]\tTrain Loss : 2.221992\n",
      "Train Eppoch : 2 [51200/60000(85%)]\tTrain Loss : 2.174224\n",
      "Train Eppoch : 2 [57600/60000(96%)]\tTrain Loss : 2.102603\n",
      "\n",
      "[EPOCH : 2], \tTest Loss : 0.0641, \tTest Accuracy : 35.06 %\n",
      "\n",
      "Train Eppoch : 3 [0/60000(0%)]\tTrain Loss : 2.101887\n",
      "Train Eppoch : 3 [6400/60000(11%)]\tTrain Loss : 2.004190\n",
      "Train Eppoch : 3 [12800/60000(21%)]\tTrain Loss : 2.050463\n",
      "Train Eppoch : 3 [19200/60000(32%)]\tTrain Loss : 1.813520\n",
      "Train Eppoch : 3 [25600/60000(43%)]\tTrain Loss : 1.904010\n",
      "Train Eppoch : 3 [32000/60000(53%)]\tTrain Loss : 1.630684\n",
      "Train Eppoch : 3 [38400/60000(64%)]\tTrain Loss : 1.707326\n",
      "Train Eppoch : 3 [44800/60000(75%)]\tTrain Loss : 1.705507\n",
      "Train Eppoch : 3 [51200/60000(85%)]\tTrain Loss : 1.343228\n",
      "Train Eppoch : 3 [57600/60000(96%)]\tTrain Loss : 1.504367\n",
      "\n",
      "[EPOCH : 3], \tTest Loss : 0.0380, \tTest Accuracy : 60.14 %\n",
      "\n",
      "Train Eppoch : 4 [0/60000(0%)]\tTrain Loss : 1.426490\n",
      "Train Eppoch : 4 [6400/60000(11%)]\tTrain Loss : 1.238831\n",
      "Train Eppoch : 4 [12800/60000(21%)]\tTrain Loss : 1.292871\n",
      "Train Eppoch : 4 [19200/60000(32%)]\tTrain Loss : 1.350428\n",
      "Train Eppoch : 4 [25600/60000(43%)]\tTrain Loss : 1.307015\n",
      "Train Eppoch : 4 [32000/60000(53%)]\tTrain Loss : 1.077820\n",
      "Train Eppoch : 4 [38400/60000(64%)]\tTrain Loss : 1.393549\n",
      "Train Eppoch : 4 [44800/60000(75%)]\tTrain Loss : 1.021191\n",
      "Train Eppoch : 4 [51200/60000(85%)]\tTrain Loss : 1.157462\n",
      "Train Eppoch : 4 [57600/60000(96%)]\tTrain Loss : 1.146427\n",
      "\n",
      "[EPOCH : 4], \tTest Loss : 0.0278, \tTest Accuracy : 69.51 %\n",
      "\n",
      "Train Eppoch : 5 [0/60000(0%)]\tTrain Loss : 0.941459\n",
      "Train Eppoch : 5 [6400/60000(11%)]\tTrain Loss : 0.968559\n",
      "Train Eppoch : 5 [12800/60000(21%)]\tTrain Loss : 0.761543\n",
      "Train Eppoch : 5 [19200/60000(32%)]\tTrain Loss : 1.077197\n",
      "Train Eppoch : 5 [25600/60000(43%)]\tTrain Loss : 1.032399\n",
      "Train Eppoch : 5 [32000/60000(53%)]\tTrain Loss : 0.768781\n",
      "Train Eppoch : 5 [38400/60000(64%)]\tTrain Loss : 0.967540\n",
      "Train Eppoch : 5 [44800/60000(75%)]\tTrain Loss : 0.967004\n",
      "Train Eppoch : 5 [51200/60000(85%)]\tTrain Loss : 0.767835\n",
      "Train Eppoch : 5 [57600/60000(96%)]\tTrain Loss : 0.937408\n",
      "\n",
      "[EPOCH : 5], \tTest Loss : 0.0240, \tTest Accuracy : 75.45 %\n",
      "\n",
      "Train Eppoch : 6 [0/60000(0%)]\tTrain Loss : 1.061247\n",
      "Train Eppoch : 6 [6400/60000(11%)]\tTrain Loss : 0.890447\n",
      "Train Eppoch : 6 [12800/60000(21%)]\tTrain Loss : 1.051202\n",
      "Train Eppoch : 6 [19200/60000(32%)]\tTrain Loss : 1.007126\n",
      "Train Eppoch : 6 [25600/60000(43%)]\tTrain Loss : 1.023779\n",
      "Train Eppoch : 6 [32000/60000(53%)]\tTrain Loss : 0.641369\n",
      "Train Eppoch : 6 [38400/60000(64%)]\tTrain Loss : 1.046847\n",
      "Train Eppoch : 6 [44800/60000(75%)]\tTrain Loss : 0.657837\n",
      "Train Eppoch : 6 [51200/60000(85%)]\tTrain Loss : 0.849663\n",
      "Train Eppoch : 6 [57600/60000(96%)]\tTrain Loss : 0.672289\n",
      "\n",
      "[EPOCH : 6], \tTest Loss : 0.0211, \tTest Accuracy : 79.73 %\n",
      "\n",
      "Train Eppoch : 7 [0/60000(0%)]\tTrain Loss : 0.823054\n",
      "Train Eppoch : 7 [6400/60000(11%)]\tTrain Loss : 0.795068\n",
      "Train Eppoch : 7 [12800/60000(21%)]\tTrain Loss : 1.344791\n",
      "Train Eppoch : 7 [19200/60000(32%)]\tTrain Loss : 0.683060\n",
      "Train Eppoch : 7 [25600/60000(43%)]\tTrain Loss : 0.534205\n",
      "Train Eppoch : 7 [32000/60000(53%)]\tTrain Loss : 0.415971\n",
      "Train Eppoch : 7 [38400/60000(64%)]\tTrain Loss : 1.018042\n",
      "Train Eppoch : 7 [44800/60000(75%)]\tTrain Loss : 0.992433\n",
      "Train Eppoch : 7 [51200/60000(85%)]\tTrain Loss : 0.832858\n",
      "Train Eppoch : 7 [57600/60000(96%)]\tTrain Loss : 0.591114\n",
      "\n",
      "[EPOCH : 7], \tTest Loss : 0.0184, \tTest Accuracy : 82.33 %\n",
      "\n",
      "Train Eppoch : 8 [0/60000(0%)]\tTrain Loss : 1.055861\n",
      "Train Eppoch : 8 [6400/60000(11%)]\tTrain Loss : 0.715404\n",
      "Train Eppoch : 8 [12800/60000(21%)]\tTrain Loss : 0.879823\n",
      "Train Eppoch : 8 [19200/60000(32%)]\tTrain Loss : 0.772503\n",
      "Train Eppoch : 8 [25600/60000(43%)]\tTrain Loss : 0.471766\n",
      "Train Eppoch : 8 [32000/60000(53%)]\tTrain Loss : 0.777002\n",
      "Train Eppoch : 8 [38400/60000(64%)]\tTrain Loss : 0.557370\n",
      "Train Eppoch : 8 [44800/60000(75%)]\tTrain Loss : 0.410545\n",
      "Train Eppoch : 8 [51200/60000(85%)]\tTrain Loss : 0.692171\n",
      "Train Eppoch : 8 [57600/60000(96%)]\tTrain Loss : 0.930120\n",
      "\n",
      "[EPOCH : 8], \tTest Loss : 0.0165, \tTest Accuracy : 84.11 %\n",
      "\n",
      "Train Eppoch : 9 [0/60000(0%)]\tTrain Loss : 0.565196\n",
      "Train Eppoch : 9 [6400/60000(11%)]\tTrain Loss : 0.796471\n",
      "Train Eppoch : 9 [12800/60000(21%)]\tTrain Loss : 0.443124\n",
      "Train Eppoch : 9 [19200/60000(32%)]\tTrain Loss : 0.805426\n",
      "Train Eppoch : 9 [25600/60000(43%)]\tTrain Loss : 0.649613\n",
      "Train Eppoch : 9 [32000/60000(53%)]\tTrain Loss : 0.375203\n",
      "Train Eppoch : 9 [38400/60000(64%)]\tTrain Loss : 0.799594\n",
      "Train Eppoch : 9 [44800/60000(75%)]\tTrain Loss : 0.532617\n",
      "Train Eppoch : 9 [51200/60000(85%)]\tTrain Loss : 0.822442\n",
      "Train Eppoch : 9 [57600/60000(96%)]\tTrain Loss : 0.831599\n",
      "\n",
      "[EPOCH : 9], \tTest Loss : 0.0148, \tTest Accuracy : 85.61 %\n",
      "\n",
      "Train Eppoch : 10 [0/60000(0%)]\tTrain Loss : 0.565389\n",
      "Train Eppoch : 10 [6400/60000(11%)]\tTrain Loss : 0.559569\n",
      "Train Eppoch : 10 [12800/60000(21%)]\tTrain Loss : 0.598044\n",
      "Train Eppoch : 10 [19200/60000(32%)]\tTrain Loss : 0.905786\n",
      "Train Eppoch : 10 [25600/60000(43%)]\tTrain Loss : 0.516719\n",
      "Train Eppoch : 10 [32000/60000(53%)]\tTrain Loss : 0.532667\n",
      "Train Eppoch : 10 [38400/60000(64%)]\tTrain Loss : 0.424890\n",
      "Train Eppoch : 10 [44800/60000(75%)]\tTrain Loss : 0.675447\n",
      "Train Eppoch : 10 [51200/60000(85%)]\tTrain Loss : 0.606094\n",
      "Train Eppoch : 10 [57600/60000(96%)]\tTrain Loss : 0.375705\n",
      "\n",
      "[EPOCH : 10], \tTest Loss : 0.0138, \tTest Accuracy : 86.58 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for Epoch in range(1, EPOCHS + 1) :\n",
    "    train(model, train_loader, optimizer ,log_interval = 200)    # (1)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)      # (2)\n",
    "    print(\"\\n[EPOCH : {}], \\tTest Loss : {:.4f}, \\tTest Accuracy : {:.2f} %\\n\".format(Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b654610",
   "metadata": {},
   "source": [
    "이론상 Dropout을 적용했을 때 일반화가 강해져 Test Accuracy가 높아지는 결과가 기대되지만, 이는 학습 데이터셋과 검증 데이터셋의 피처 및 레이블의 분포 간 많은 차이가 있을 때 유효함.  \n",
    "MNIST 데이터셋은 학습 데이터와 검증 데이터 간 많은 차이가 발생하지 않아 오히려 성능이 조금 하락할 수도 있음. 하지만 Epoch을 늘려 추가로 학습하면 성능이 좋아짐.  \n",
    "Dropout은 보통 ReLU() 비선형 함수와 잘 어울림. 다음 예제에서는 비선형 함수를 Sigmoid()에서 ReLU()로 변경했을 때 Dropout의 효과를 살펴보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baafee6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
