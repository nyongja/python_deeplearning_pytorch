{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247ae8b4",
   "metadata": {},
   "source": [
    "## 예제 3-5) 사람의 손글씨 데이터인 MNIST를 이용해 MLP 설계할 때 Dropout & ReLU & Batch Normalization & He Uniform Initialization & Adam 적용해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f904cf4e",
   "metadata": {},
   "source": [
    "다양한 요소를 조절해봤는데, 그중 가장 중요한 것이 바로 학습에 이용되는 Optimizer.  \n",
    "Optimizer 중에서도 가장 자주 이용되는 Adamdmf 이용해 실습해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4af4c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn                           \n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d19b489f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version :  1.9.0  Device :  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyongja/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() :\n",
    "    DEVICE = torch.device('cuda')\n",
    "else :\n",
    "    DEVICE = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version : ', torch.__version__, ' Device : ', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "762a5b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a6f3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyongja/anaconda3/envs/pytorch_study/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                              train = True,\n",
    "                              download = True,\n",
    "                              transform = transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                              train = False,\n",
    "                              transform = transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a046fc2",
   "metadata": {},
   "source": [
    "nn.BatchNorm( ) 함수를 적용하는 부분은 논문 / 코드에 따라 Activation Function 전, 후가 달라질 수 있음.  \n",
    "이 예제에서는 이전에 적용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbe25794",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module) : \n",
    "    def __init__(self) :  \n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5\n",
    "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9915057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b5f7f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m) :                              \n",
    "    if isinstance(m, nn.Linear) :                 \n",
    "        init.kaiming_uniform_(m.weight.data)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c414c945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(DEVICE)\n",
    "model.apply(weight_init)                                                  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)  # (1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411e1740",
   "metadata": {},
   "source": [
    "(1) optimizer 정의는 한 줄만 변경하면 됨. Adam은 RMSProp + Momentum으로 다양한 optimizer 중 기본적으로 자주 사용 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "174cda97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, log_interval) :\n",
    "    model.train()            \n",
    "    for batch_idx, (image, label) in enumerate(train_loader) :\n",
    "        image = image.to(DEVICE)                                                               \n",
    "        label = label.to(DEVICE)                                                               \n",
    "        optimizer.zero_grad()                                                                  \n",
    "        output = model(image)                                                                  \n",
    "        loss = criterion(output, label)                                                        \n",
    "        loss.backward()                                                                        \n",
    "        optimizer.step()                                                                       \n",
    "        \n",
    "        if batch_idx % log_interval == 0 :\n",
    "            print(\"Train Eppoch : {} [{}/{}({:.0f}%)]\\tTrain Loss : {:.6f}\".format(\n",
    "                Epoch, batch_idx * len(image), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90930a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader) :\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad() :\n",
    "        for image, label in test_loader :\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)  \n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item() \n",
    "            \n",
    "    test_loss /= len(test_loader.dataset) \n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset) \n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d262451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Eppoch : 1 [0/60000(0%)]\tTrain Loss : 2.841109\n",
      "Train Eppoch : 1 [6400/60000(11%)]\tTrain Loss : 0.273090\n",
      "Train Eppoch : 1 [12800/60000(21%)]\tTrain Loss : 0.233896\n",
      "Train Eppoch : 1 [19200/60000(32%)]\tTrain Loss : 0.694295\n",
      "Train Eppoch : 1 [25600/60000(43%)]\tTrain Loss : 0.256474\n",
      "Train Eppoch : 1 [32000/60000(53%)]\tTrain Loss : 0.369895\n",
      "Train Eppoch : 1 [38400/60000(64%)]\tTrain Loss : 0.408795\n",
      "Train Eppoch : 1 [44800/60000(75%)]\tTrain Loss : 0.228371\n",
      "Train Eppoch : 1 [51200/60000(85%)]\tTrain Loss : 0.586753\n",
      "Train Eppoch : 1 [57600/60000(96%)]\tTrain Loss : 0.097312\n",
      "\n",
      "[EPOCH : 1], \tTest Loss : 0.0039, \tTest Accuracy : 96.03 %\n",
      "\n",
      "Train Eppoch : 2 [0/60000(0%)]\tTrain Loss : 0.442182\n",
      "Train Eppoch : 2 [6400/60000(11%)]\tTrain Loss : 0.437141\n",
      "Train Eppoch : 2 [12800/60000(21%)]\tTrain Loss : 0.319928\n",
      "Train Eppoch : 2 [19200/60000(32%)]\tTrain Loss : 0.415676\n",
      "Train Eppoch : 2 [25600/60000(43%)]\tTrain Loss : 0.146514\n",
      "Train Eppoch : 2 [32000/60000(53%)]\tTrain Loss : 0.048029\n",
      "Train Eppoch : 2 [38400/60000(64%)]\tTrain Loss : 0.581138\n",
      "Train Eppoch : 2 [44800/60000(75%)]\tTrain Loss : 0.170076\n",
      "Train Eppoch : 2 [51200/60000(85%)]\tTrain Loss : 0.118025\n",
      "Train Eppoch : 2 [57600/60000(96%)]\tTrain Loss : 0.101330\n",
      "\n",
      "[EPOCH : 2], \tTest Loss : 0.0032, \tTest Accuracy : 96.68 %\n",
      "\n",
      "Train Eppoch : 3 [0/60000(0%)]\tTrain Loss : 0.439379\n",
      "Train Eppoch : 3 [6400/60000(11%)]\tTrain Loss : 0.101597\n",
      "Train Eppoch : 3 [12800/60000(21%)]\tTrain Loss : 0.126028\n",
      "Train Eppoch : 3 [19200/60000(32%)]\tTrain Loss : 0.331716\n",
      "Train Eppoch : 3 [25600/60000(43%)]\tTrain Loss : 0.378339\n",
      "Train Eppoch : 3 [32000/60000(53%)]\tTrain Loss : 0.381852\n",
      "Train Eppoch : 3 [38400/60000(64%)]\tTrain Loss : 0.090787\n",
      "Train Eppoch : 3 [44800/60000(75%)]\tTrain Loss : 0.197817\n",
      "Train Eppoch : 3 [51200/60000(85%)]\tTrain Loss : 0.071983\n",
      "Train Eppoch : 3 [57600/60000(96%)]\tTrain Loss : 0.326704\n",
      "\n",
      "[EPOCH : 3], \tTest Loss : 0.0030, \tTest Accuracy : 97.06 %\n",
      "\n",
      "Train Eppoch : 4 [0/60000(0%)]\tTrain Loss : 0.781967\n",
      "Train Eppoch : 4 [6400/60000(11%)]\tTrain Loss : 0.209466\n",
      "Train Eppoch : 4 [12800/60000(21%)]\tTrain Loss : 0.338237\n",
      "Train Eppoch : 4 [19200/60000(32%)]\tTrain Loss : 0.319799\n",
      "Train Eppoch : 4 [25600/60000(43%)]\tTrain Loss : 0.287965\n",
      "Train Eppoch : 4 [32000/60000(53%)]\tTrain Loss : 0.111300\n",
      "Train Eppoch : 4 [38400/60000(64%)]\tTrain Loss : 0.250714\n",
      "Train Eppoch : 4 [44800/60000(75%)]\tTrain Loss : 0.126765\n",
      "Train Eppoch : 4 [51200/60000(85%)]\tTrain Loss : 0.051939\n",
      "Train Eppoch : 4 [57600/60000(96%)]\tTrain Loss : 0.226568\n",
      "\n",
      "[EPOCH : 4], \tTest Loss : 0.0030, \tTest Accuracy : 96.89 %\n",
      "\n",
      "Train Eppoch : 5 [0/60000(0%)]\tTrain Loss : 0.092188\n",
      "Train Eppoch : 5 [6400/60000(11%)]\tTrain Loss : 0.449260\n",
      "Train Eppoch : 5 [12800/60000(21%)]\tTrain Loss : 0.165204\n",
      "Train Eppoch : 5 [19200/60000(32%)]\tTrain Loss : 0.068823\n",
      "Train Eppoch : 5 [25600/60000(43%)]\tTrain Loss : 0.204067\n",
      "Train Eppoch : 5 [32000/60000(53%)]\tTrain Loss : 0.308247\n",
      "Train Eppoch : 5 [38400/60000(64%)]\tTrain Loss : 0.294681\n",
      "Train Eppoch : 5 [44800/60000(75%)]\tTrain Loss : 0.094091\n",
      "Train Eppoch : 5 [51200/60000(85%)]\tTrain Loss : 0.202359\n",
      "Train Eppoch : 5 [57600/60000(96%)]\tTrain Loss : 0.518627\n",
      "\n",
      "[EPOCH : 5], \tTest Loss : 0.0026, \tTest Accuracy : 97.45 %\n",
      "\n",
      "Train Eppoch : 6 [0/60000(0%)]\tTrain Loss : 0.236189\n",
      "Train Eppoch : 6 [6400/60000(11%)]\tTrain Loss : 0.064249\n",
      "Train Eppoch : 6 [12800/60000(21%)]\tTrain Loss : 0.319116\n",
      "Train Eppoch : 6 [19200/60000(32%)]\tTrain Loss : 0.096748\n",
      "Train Eppoch : 6 [25600/60000(43%)]\tTrain Loss : 0.148656\n",
      "Train Eppoch : 6 [32000/60000(53%)]\tTrain Loss : 0.166618\n",
      "Train Eppoch : 6 [38400/60000(64%)]\tTrain Loss : 0.040411\n",
      "Train Eppoch : 6 [44800/60000(75%)]\tTrain Loss : 0.091119\n",
      "Train Eppoch : 6 [51200/60000(85%)]\tTrain Loss : 0.120550\n",
      "Train Eppoch : 6 [57600/60000(96%)]\tTrain Loss : 0.007148\n",
      "\n",
      "[EPOCH : 6], \tTest Loss : 0.0023, \tTest Accuracy : 97.95 %\n",
      "\n",
      "Train Eppoch : 7 [0/60000(0%)]\tTrain Loss : 0.213975\n",
      "Train Eppoch : 7 [6400/60000(11%)]\tTrain Loss : 0.117879\n",
      "Train Eppoch : 7 [12800/60000(21%)]\tTrain Loss : 0.217816\n",
      "Train Eppoch : 7 [19200/60000(32%)]\tTrain Loss : 0.005380\n",
      "Train Eppoch : 7 [25600/60000(43%)]\tTrain Loss : 0.416876\n",
      "Train Eppoch : 7 [32000/60000(53%)]\tTrain Loss : 0.272910\n",
      "Train Eppoch : 7 [38400/60000(64%)]\tTrain Loss : 0.242056\n",
      "Train Eppoch : 7 [44800/60000(75%)]\tTrain Loss : 0.146942\n",
      "Train Eppoch : 7 [51200/60000(85%)]\tTrain Loss : 0.089876\n",
      "Train Eppoch : 7 [57600/60000(96%)]\tTrain Loss : 0.037645\n",
      "\n",
      "[EPOCH : 7], \tTest Loss : 0.0025, \tTest Accuracy : 97.59 %\n",
      "\n",
      "Train Eppoch : 8 [0/60000(0%)]\tTrain Loss : 0.145313\n",
      "Train Eppoch : 8 [6400/60000(11%)]\tTrain Loss : 0.496826\n",
      "Train Eppoch : 8 [12800/60000(21%)]\tTrain Loss : 0.117524\n",
      "Train Eppoch : 8 [19200/60000(32%)]\tTrain Loss : 0.068266\n",
      "Train Eppoch : 8 [25600/60000(43%)]\tTrain Loss : 0.362787\n",
      "Train Eppoch : 8 [32000/60000(53%)]\tTrain Loss : 0.039818\n",
      "Train Eppoch : 8 [38400/60000(64%)]\tTrain Loss : 0.092999\n",
      "Train Eppoch : 8 [44800/60000(75%)]\tTrain Loss : 0.248242\n",
      "Train Eppoch : 8 [51200/60000(85%)]\tTrain Loss : 0.189218\n",
      "Train Eppoch : 8 [57600/60000(96%)]\tTrain Loss : 0.190906\n",
      "\n",
      "[EPOCH : 8], \tTest Loss : 0.0024, \tTest Accuracy : 97.74 %\n",
      "\n",
      "Train Eppoch : 9 [0/60000(0%)]\tTrain Loss : 0.262321\n",
      "Train Eppoch : 9 [6400/60000(11%)]\tTrain Loss : 0.019575\n",
      "Train Eppoch : 9 [12800/60000(21%)]\tTrain Loss : 0.064479\n",
      "Train Eppoch : 9 [19200/60000(32%)]\tTrain Loss : 0.069865\n",
      "Train Eppoch : 9 [25600/60000(43%)]\tTrain Loss : 0.113062\n",
      "Train Eppoch : 9 [32000/60000(53%)]\tTrain Loss : 0.041948\n",
      "Train Eppoch : 9 [38400/60000(64%)]\tTrain Loss : 0.044483\n",
      "Train Eppoch : 9 [44800/60000(75%)]\tTrain Loss : 0.189755\n",
      "Train Eppoch : 9 [51200/60000(85%)]\tTrain Loss : 0.066211\n",
      "Train Eppoch : 9 [57600/60000(96%)]\tTrain Loss : 0.066513\n",
      "\n",
      "[EPOCH : 9], \tTest Loss : 0.0021, \tTest Accuracy : 97.95 %\n",
      "\n",
      "Train Eppoch : 10 [0/60000(0%)]\tTrain Loss : 0.425605\n",
      "Train Eppoch : 10 [6400/60000(11%)]\tTrain Loss : 0.049624\n",
      "Train Eppoch : 10 [12800/60000(21%)]\tTrain Loss : 0.014738\n",
      "Train Eppoch : 10 [19200/60000(32%)]\tTrain Loss : 0.033997\n",
      "Train Eppoch : 10 [25600/60000(43%)]\tTrain Loss : 0.090748\n",
      "Train Eppoch : 10 [32000/60000(53%)]\tTrain Loss : 0.012803\n",
      "Train Eppoch : 10 [38400/60000(64%)]\tTrain Loss : 0.015015\n",
      "Train Eppoch : 10 [44800/60000(75%)]\tTrain Loss : 0.262183\n",
      "Train Eppoch : 10 [51200/60000(85%)]\tTrain Loss : 0.067815\n",
      "Train Eppoch : 10 [57600/60000(96%)]\tTrain Loss : 0.366600\n",
      "\n",
      "[EPOCH : 10], \tTest Loss : 0.0022, \tTest Accuracy : 97.88 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for Epoch in range(1, EPOCHS + 1) :\n",
    "    train(model, train_loader, optimizer ,log_interval = 200)    # (1)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)      # (2)\n",
    "    print(\"\\n[EPOCH : {}], \\tTest Loss : {:.4f}, \\tTest Accuracy : {:.2f} %\\n\".format(Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baafee6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
